# Ultralytics 🚀 AGPL-3.0 license

# Parameters
nc: 10  # 类别数
scales:
  n: [0.50, 0.25, 1024]  # depth, width, max_channels

# Backbone with Attention
backbone:
  # 第0层: 输入通道3 → 输出16
  - [-1, 1, Conv, [16, 3, 2]]          # 0-P1/2 (3,16,3,2)
  # 第1层: 输入16 → 输出32
  - [-1, 1, Conv, [32, 3, 2]]          # 1-P2/4 (16,32,3,2)
  # 第2层: C3k2模块 → 输出64
  - [-1, 2, C3k2, [64, False, 0.25]]   # 2 (32,64)
  # increase第3层: SE注意力 → 输入64（必须与上一层输出一致）
  - [-1, 1, SEAttention, [16, 4]]     # 3 (64) reduction=16 → 64/16=4
  # 第4层: 下采样 → 输出128
  - [-1, 1, Conv, [128, 3, 2]]         # 4-P3/8 (64,128,3,2)
  # 第5层: C3k2模块 → 输出256
  - [-1, 2, C3k2, [256, False, 0.25]]  # 5 (128,256)
  # increase第6层: CBAM注意力 → 输入64通道，reduction=16（可省略第二个参数）&#8203;:contentReference[oaicite:1]{index=1}
  - [-1, 1, CBAM, [64, 16]]               # 6 (256)
  # 第7层: 下采样 → 输出512
  - [-1, 1, Conv, [512, 3, 2]]         # 7-P4/16 (256,512,3,2)
  # 第8层: C3k2模块 → 输出512
  - [-1, 2, C3k2, [512, True]]         # 8 (512,512)
  # increase 第9层: SE注意力 → 输入输入64通道，r=16 → hidden=64//16=4 :contentReference[oaicite:2]{index=2}
  - [-1, 1, SEAttention, [128, 16]]    # 9 (512) reduction=32 → 512/32=16
  # 第10层: 下采样 → 输出1024
  - [-1, 1, Conv, [1024, 3, 2]]        # 10-P5/32 (512,1024,3,2)
  # 第11层: C3k2模块 → 输出1024
  - [-1, 2, C3k2, [1024, True]]        # 11 (1024,1024)
  # 第12层: C2PSA模块 → 输出1024
  - [-1, 2, C2PSA, [1024]]             # 12 (1024)
  # increase 第13层: CBAM注意力 → 输入1024
  - [-1, 1, CBAM, [256, 16]]              # 13 (1024)

# Enhanced Head
head:
  # 第14层: 过渡卷积 → 输出1024
  - [-1, 1, Conv, [1024, 3, 1]]        # 14 (1024,1024,3,1)
  # increase 第15层: SE注意力 → 输入1024
  - [-1, 1, SEAttention, [256, 64]]   # 15 (1024) reduction=64 → 1024/64=16
  # 第16层: 分类头 → 输出nc=10
  - [-1, 1, Classify, [nc]]            # 16 (1024,10)
